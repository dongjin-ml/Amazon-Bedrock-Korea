{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df503520-00ac-434c-aea7-458add4857d9",
   "metadata": {},
   "source": [
    "## Post Call Analytics (PCA) Using Amazon Bedrock\n",
    "\n",
    "Welcome to this training module on post-call analytics use cases using Amazon Bedrock. \n",
    "\n",
    "As businesses continue to interact with customers through various channels, it becomes increasingly important to analyze these interactions to gain insights into customer behavior and preferences. Post-call analytics is one such method that involves analyzing customer interactions after the call has ended. The use of large language models can greatly enhance the effectiveness of post-call analytics by enabling more accurate sentiment analysis, identifying specific customer needs and preferences, and improving overall customer experience. \n",
    "\n",
    "In this sample notebook, we will explore following topics to demonstrate the various benefits of using Bedrock for post-call analytics and businesses gain a competitive edge in the modern marketplace.\n",
    "\n",
    "- Choice of LLM models in Bedrock (Titan Text and Anthropic Claude)\n",
    "- One model handling multiple PCA tasks\n",
    "- Handling long call transcripts\n",
    "- [Stretch] Architecture pattern for production workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d1935-a875-4938-9c1f-ac171dedb608",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "Install and upgrade the packages required to run the sample code. <BR>\n",
    "**Note: you may need to restart the kernel to use updated packages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4af435-2f0b-47cf-849c-fc16f8189400",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9547efff-ad04-471e-af4d-d7988781d23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain version check: 0.0.232\n",
      "boto3 version check: 1.26.162\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import PromptTemplate\n",
    "import boto3\n",
    "\n",
    "print(f\"langchain version check: {langchain.__version__}\")\n",
    "print(f\"boto3 version check: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8351fe-cf0b-46ac-bd71-fac01c47f9fd",
   "metadata": {},
   "source": [
    "# Load transcript files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4c458a-7cf2-4029-90fd-a641f1556dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_files = [\"./call_transcripts/negative-refund.txt\", \n",
    "                    \"./call_transcripts/neutral-short.txt\",\n",
    "                    \"./call_transcripts/positive-partial-refund.txt\"]\n",
    "\n",
    "transcripts = []\n",
    "\n",
    "for file_name in transcript_files:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        transcripts.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9237aa06-a992-4713-bd21-6b82b0d763a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcript #1: timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling our retail support line. My name is ABC. How can I assist you today?\n",
      "\n",
      "Customer: Yes, I have received a defective product, and I am extremely angry about it! This is unacceptable, and I want it resolved immediately!\n",
      "\n",
      "Agent: I'm sorry\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #2: timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up y\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #3: timestamp: 2022-12-28 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling [Retailer], my name is [Agent Name]. How may I assist you today?\n",
      "\n",
      "Customer: Hi, I wanted to check on the status of my order. It was supposed to arrive today, but I haven't received it yet.\n",
      "\n",
      "Agent: I'm sorry to hear that. Can I have \n",
      "\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, trans in enumerate(transcripts):\n",
    "    print(f\"transcript #{i+1}: {trans[:300]}\\n\")\n",
    "    print(\"====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74faf4ac-d5ec-42b7-8f6b-b02988898b10",
   "metadata": {},
   "source": [
    "# Post Call Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27963a63-4eb6-4b40-a88c-9396ad202c7e",
   "metadata": {},
   "source": [
    "## Choice of models in Bedrock\n",
    "Choose FMs from Amazon, AI21 Labs and Anthropic to find the right FM for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94025e02-4dc2-48a4-8324-171b906769d4",
   "metadata": {},
   "source": [
    "**Select region: \"us-east-1\"(M1), \"us-west-2\"(M2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef184e65-8202-42f2-86dc-734739d8e1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_region = \"us-east-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14312ebe-27a4-4f83-9971-1f49637ad741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bedrock_region == \"us-east-1\":    \n",
    "    bedrock_config = {\n",
    "        \"region_name\":bedrock_region,\n",
    "        \"endpoint_url\":\"https://bedrock.us-east-1.amazonaws.com\"\n",
    "    }\n",
    "elif bedrock_region == \"us-west-2\":  \n",
    "    bedrock_config = {\n",
    "        \"region_name\":bedrock_region,\n",
    "        \"endpoint_url\":\"https://prod.us-west-2.frontend.bedrock.aws.dev\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a00ed0d-3400-403f-a40d-60bf22204ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name=bedrock_config[\"region_name\"],\n",
    "    endpoint_url=bedrock_config[\"endpoint_url\"]\n",
    ")\n",
    "\n",
    "bedrock_models = {\n",
    "    \"Claude\" : \"anthropic.claude-v1\",\n",
    "    \"TitanText\": \"amazon.titan-tg1-large\", \n",
    "    \"Claude-instant\":\"anthropic.claude-instant-v1\"\n",
    "}\n",
    "\n",
    "max_tokens = {\n",
    "    \"Claude\" : 12000,\n",
    "    \"TitanText\": 4096,\n",
    "    \"Claude-instant\": 9000\n",
    "}\n",
    "\n",
    "max_tokens = {\"Claude\" : 120, \"TitanText\": 130, \"Claude-instant\": 120}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4355a638-0ecd-42e2-a98b-b62ae3022b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3171a9f-feac-4c3a-8940-7f9bb40e5014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose one of the bedrock model\n",
    "model = \"TitanText\" # \"Claude\", \"TitanText\", \"Claude-instant\"\n",
    "if model in [\"Claude\", \"Claude-instant\"]:\n",
    "    llm = Bedrock(\n",
    "        model_id=bedrock_models[model],\n",
    "        client=bedrock,\n",
    "        model_kwargs={\n",
    "            \"max_tokens_to_sample\":200,\n",
    "            \"stop_sequences\":[],\n",
    "            \"temperature\":0,\n",
    "            \"top_p\":0.9\n",
    "        },\n",
    "        endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev'\n",
    "    )\n",
    "elif model == \"TitanText\":\n",
    "    llm = Bedrock(\n",
    "        model_id=bedrock_models[model],\n",
    "        client=bedrock,\n",
    "        model_kwargs={\n",
    "            \"maxTokenCount\":4096,\n",
    "            \"stopSequences\":[],\n",
    "            \"temperature\":0,\n",
    "            \"topP\":0.9\n",
    "        },\n",
    "        endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128dd79-f560-4278-82e7-19d1b2a16eac",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "In this notebook, we'll be performing four different analyses(**Summary, Sentiment, Intent and Resolution**), and we'll need a template for each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2bd9304-f0f3-46f2-9414-58fbdc85b132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template = \"\"\"Analyze the retail support call transcript below. Provide a detail summary of the conversation in complete sentence:\n",
    "\n",
    "context: \"{transcript}\"\n",
    "\n",
    "summary:\"\"\"\n",
    "\n",
    "# What is the sentiment of the conversation: \"\"\"\n",
    "sentiment_template = \"\"\"\n",
    "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
    "[\"POSITIVE\", \"NEUTRAL\",\"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
    "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "sentiment: \"\"\"\n",
    "\n",
    "intent_template = \"\"\"This is a intent classification program. What is the purpose of the customer call use following \n",
    "classes [\"SHIPMENT_DELAY\", \"PRODUCT_DEFECT\", \"ACCOUNT_QUESTION\"]. Classify the conversation into one \n",
    "and exact one of these classes. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class. \n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Answer in one word, why is customer calling today: \"\"\"\n",
    "\n",
    "resolution_template = \"\"\"This is a resolution classification program. How did the agent solved the issue use following \n",
    "classes [\"FULL_REFUND\", \"PARTIAL_REFUND\",  \"QUESTION_ANSWERED\", \"UNRESOLVED\"]. classify the conversation into one \n",
    "and exact one of these classes. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Answer in one word, how did the agent resolve the customer question or issue: \"\"\"\n",
    "\n",
    "topic_template = \"\"\"This is topic identification program. What specific topic agent observed during the call. If you don't know, please say \"I don't know\". Do not try to make up.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Topic: \"\"\"\n",
    "\n",
    "escalation_template = \"\"\"This is escalation classification program. Did customer asked for escalation during the call. use following classes [\"YES\", \"NO\",  \"UNKNOWN\"]. Classify the conversation into one \n",
    "and exact one of these classes. Answer in one word without any explaination. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Escalation: \"\"\"\n",
    "\n",
    "\n",
    "holdup_template = \"\"\"This is delay identification program. Did agent put customer on hold during the call. \n",
    "If Yes, provide top reason concisely. If no wait or hold, then just say \"No Hold-up\".\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Top Reason: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f54dae-6f83-4820-9764-d8e9b943f8cc",
   "metadata": {},
   "source": [
    "## Generate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d5fe77-773c-4b3c-b146-068be5ddade0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_analysis(llm, transcript, max_tokens=50, template=\"\"):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    \n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    print (analysis_prompt)\n",
    "        \n",
    "    analysis = llm(analysis_prompt)\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedaaac-c4ea-40b5-a4e8-02b6f913ebc1",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d19126a2-0c99-4c39-b49e-9f9f6c3b4a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the retail support call transcript below. Provide a detail summary of the conversation in complete sentence:\n",
      "\n",
      "context: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe customer called to check their account balance and the retail support representative provided the current balance of $567.89. The customer was satisfied with the response and there was no further discussion or assistance required. The call ended shortly after with a thank you from both parties.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=summary_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b317a2-f6ca-4c4d-981f-a8a29745e1ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ca68894-4069-41ac-a427-ab30190a5901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
      "[\"POSITIVE\", \"NEUTRAL\",\"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
      "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class.\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "sentiment: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NEUTRAL'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=sentiment_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573494c0-4a7a-47c1-ae43-9e0aa741949f",
   "metadata": {},
   "source": [
    "### Intent Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ce3de55-2eaf-4e2b-b98f-c6950324520e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a intent classification program. What is the purpose of the customer call use following \n",
      "classes [\"SHIPMENT_DELAY\", \"PRODUCT_DEFECT\", \"ACCOUNT_QUESTION\"]. Classify the conversation into one \n",
      "and exact one of these classes. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class. \n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "Answer in one word, why is customer calling today: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ACCOUNT_QUESTION'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=intent_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d260018-1cb4-43be-8184-017ab1d5b103",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aef3a5d7-43d9-49c7-87aa-90fb123facdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a resolution classification program. How did the agent solved the issue use following \n",
      "classes [\"FULL_REFUND\", \"PARTIAL_REFUND\",  \"QUESTION_ANSWERED\", \"UNRESOLVED\"]. classify the conversation into one \n",
      "and exact one of these classes. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "Answer in one word, how did the agent resolve the customer question or issue: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'QUESTION_ANSWERED'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=resolution_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3e7f27e-1823-462e-9a0a-576a69a6d50f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is topic identification program. What specific topic agent observed during the call. If you don't know, please say \"I don't know\". Do not try to make up.\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "Topic: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Checking account balance'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=topic_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3ea98fb-aac2-4cc9-8d2c-7d3e5ad1ffaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is escalation classification program. Did customer asked for escalation during the call. use following classes [\"YES\", \"NO\",  \"UNKNOWN\"]. Classify the conversation into one \n",
      "and exact one of these classes. Answer in one word without any explaination. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "Escalation: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NO'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=escalation_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "877be6ad-546e-4919-b434-468076faaff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is delay identification program. Did agent put customer on hold during the call. \n",
      "If Yes, provide top reason concisely. If no wait or hold, then just say \"No Hold-up\".\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "Top Reason: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No Hold-up'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=holdup_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988ceca-2edb-4bff-93fd-b6730c0d54e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling long call transcripts\n",
    "We'll cover how to handle long transcripts that exceed the limits of the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b86ca5e-971b-4034-a5ed-99f4292227bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if exceeds titan limit of 4000 tokens; Chunk it up\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eb786d0-c847-4f37-bbc2-54b2d33e8f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Analysis\n",
    "def generate_analysis(llm, transcript, template=\"\", model=\"Claude\"):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    \n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    \n",
    "    num_tokens = llm.get_num_tokens(analysis_prompt)\n",
    "    print (f'prompt has almost {num_tokens} tokens \\n')\n",
    "    \n",
    "    print (\"max_tokens[model]\", max_tokens[model])\n",
    "    if num_tokens > max_tokens[model]:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=20\n",
    "        )\n",
    "        docs = text_splitter.create_documents([transcript])\n",
    "        \n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "        print(\n",
    "            f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\"\n",
    "        )\n",
    "        summary_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=True) # map_reduce\n",
    "        \n",
    "        transcript = summary_chain.run(docs)\n",
    "                \n",
    "    analysis_prompt = prompt.format(transcript=transcript) \n",
    "    analysis = llm(analysis_prompt)\n",
    "    \n",
    "    return (analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ef1e7-4687-43a0-9935-2d4ac263285b",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c23fa62-0898-4bb7-9991-d2d4705e0a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt has almost 219 tokens \n",
      "\n",
      "max_tokens[model] 120\n",
      "Now we have 2 documents and the first one has 140 tokens\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "The customer calls to check their account balance and the retail support representative provides the current balance of $567.89.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The customer inquired about their account balance, and the retail support representative provided the information.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=summary_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8790f920-bf59-4828-8aaa-f4f511f0e10a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m146.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187afa8f-908c-4821-ac77-29eac133d6c9",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76584745-17c0-445b-b852-a6779f110ff7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
      "[\"POSITIVE\", \"NEUTRAL\",\"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
      "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class.\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "sentiment: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error raised by bedrock service: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mThrottlingException\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/bedrock.py:189\u001b[0m, in \u001b[0;36mBedrock._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontentType\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     text \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(provider, response)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    963\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mThrottlingException\u001b[0m: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscript\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranscripts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentiment_template\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 8\u001b[0m, in \u001b[0;36mgenerate_analysis\u001b[0;34m(llm, transcript, max_tokens, template)\u001b[0m\n\u001b[1;32m      5\u001b[0m analysis_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(transcript\u001b[38;5;241m=\u001b[39mtranscript)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (analysis_prompt)\n\u001b[0;32m----> 8\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalysis_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:427\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    437\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:279\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[1;32m    276\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    277\u001b[0m         dumpd(\u001b[38;5;28mself\u001b[39m), prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[1;32m    278\u001b[0m     )\n\u001b[0;32m--> 279\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:223\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    222\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    224\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:210\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    202\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 210\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    219\u001b[0m         )\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:602\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m    601\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 602\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/bedrock.py:195\u001b[0m, in \u001b[0;36mBedrock._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     text \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(provider, response)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by bedrock service: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     text \u001b[38;5;241m=\u001b[39m enforce_stop_tokens(text, stop)\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by bedrock service: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again."
     ]
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=sentiment_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee26221-a4e5-4e3e-896e-5c9d40080900",
   "metadata": {},
   "source": [
    "### Intent Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0bf4f-cdb1-4f3d-a685-f6966f45223e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm,transcript=transcripts[1], template=intent_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffde34e-8fec-4a7d-86be-ca6a580d151d",
   "metadata": {},
   "source": [
    "### Resolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cc2ac-4e00-440a-9056-9809d4064cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=resolution_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89141b59-f8cb-4a0c-b5f3-2d076a09f949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
