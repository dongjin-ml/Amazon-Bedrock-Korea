{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df503520-00ac-434c-aea7-458add4857d9",
   "metadata": {},
   "source": [
    "## Post Call Analytics (PCA) Using Amazon Bedrock\n",
    "\n",
    "Welcome to this training module on post-call analytics use cases using Amazon Bedrock. \n",
    "\n",
    "As businesses continue to interact with customers through various channels, it becomes increasingly important to analyze these interactions to gain insights into customer behavior and preferences. Post-call analytics is one such method that involves analyzing customer interactions after the call has ended. The use of large language models can greatly enhance the effectiveness of post-call analytics by enabling more accurate sentiment analysis, identifying specific customer needs and preferences, and improving overall customer experience. \n",
    "\n",
    "In this sample notebook, we will explore following topics to demonstrate the various benefits of using Bedrock for post-call analytics and businesses gain a competitive edge in the modern marketplace.\n",
    "\n",
    "- Choice of LLM models in Bedrock (Titan Text and Anthropic Claude)\n",
    "- One model handling multiple PCA tasks\n",
    "- Handling long call transcripts\n",
    "- [Stretch] Architecture pattern for production workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d1935-a875-4938-9c1f-ac171dedb608",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "Install and upgrade the packages required to run the sample code. <BR>\n",
    "**Note: you may need to restart the kernel to use updated packages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4af435-2f0b-47cf-849c-fc16f8189400",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9547efff-ad04-471e-af4d-d7988781d23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain version check: 0.0.256\n",
      "boto3 version check: 1.28.21\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import PromptTemplate\n",
    "import boto3\n",
    "\n",
    "print(f\"langchain version check: {langchain.__version__}\")\n",
    "print(f\"boto3 version check: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8351fe-cf0b-46ac-bd71-fac01c47f9fd",
   "metadata": {},
   "source": [
    "# Load transcript files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ab4c458a-7cf2-4029-90fd-a641f1556dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_files = [\n",
    "    #\"./call_transcripts/negative-refund.txt\", \n",
    "    #\"./call_transcripts/neutral-short.txt\",\n",
    "    #\"./call_transcripts/positive-partial-refund.txt\",\n",
    "    #\"./call_transcripts/aws.txt\",\n",
    "    \"./call_transcripts/negative-refund-ko.txt\",\n",
    "    \"./call_transcripts/neutral-short-ko.txt\",\n",
    "    \"./call_transcripts/positive-partial-refund-ko.txt\",\n",
    "    \"./call_transcripts/aws-ko.txt\"\n",
    "]\n",
    "\n",
    "transcripts = []\n",
    "\n",
    "for file_name in transcript_files:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        transcripts.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9237aa06-a992-4713-bd21-6b82b0d763a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcript #1: timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "상담원: 리테일 지원 라인에 전화해 주셔서 감사합니다. 제 이름은 ABC입니다. 오늘 무엇을 도와드릴까요?\n",
      "\n",
      "고객님: 예, 결함이 있는 제품을 받았는데 매우 화가 납니다! 이것은 용납할 수 없는 일이며 즉시 해결하고 싶습니다!\n",
      "\n",
      "상담원: 네: 결함이 있는 제품을 받으셨다니 유감입니다. 어떤 문제인지 알려주시겠어요?\n",
      "\n",
      "고객: 네: 제가 받은 제품이 파손되어 사용할 수 없습니다. 많은 돈을 주고 샀는데 이제 사용할 수도 없습니다! 이것은 용납할 수 없는 일이며 지금 \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #2: timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "고객: 안녕하세요, 제 계정의 잔액을 확인하고 싶습니다.\n",
      "\n",
      "상담원: 물론이죠! 계정에 연결된 계좌 번호나 전화번호를 알려주실 수 있나요?\n",
      "\n",
      "고객: 네: 제 전화번호는 (123) 456-7890입니다.\n",
      "\n",
      "상담원: 네, 감사합니다. 계정을 불러올게요. 현재 잔액이 $567.89인 것 같습니다.\n",
      "\n",
      "고객: 네, 좋아요. 감사합니다.\n",
      "\n",
      "상담원: 천만에요! 오늘 또 도와드릴 일이 있나요?\n",
      "\n",
      "고객: 아니요, 그게 다입니다. 감사합니다.\n",
      "\n",
      "상담원: 문제 없습니다. 전화해 주셔서\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #3: timestamp: 2022-12-28 08:26:49.219717\n",
      "\n",
      "상담원: 소매업체]에 전화해 주셔서 감사합니다. 제 이름은 [상담원 이름]입니다. 오늘은 무엇을 도와드릴까요?\n",
      "\n",
      "고객: 안녕하세요, 주문 상태를 확인하고 싶어서요. 오늘 도착하기로 되어 있었는데 아직 받지 못했습니다.\n",
      "\n",
      "상담원: 유감입니다. 주문 번호를 알려주시겠습니까?\n",
      "\n",
      "고객: 네, 123456입니다.\n",
      "\n",
      "상담원: 네: 감사합니다. 제가 확인해 보겠습니다. 창고에서 예기치 않은 문제가 발생하여 주문이 며칠 지연된 것 같습니다. 불편을 드려 죄송합니다.\n",
      "\n",
      "고객: 괜\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "transcript #4: AWS란 무엇인가요? AWS 또는 Amazon Web Services는 공용 인터넷을 통해 액세스할 수 있는 다양한 컴퓨팅 서비스를 제공하는 클라우드 서비스 제공업체입니다.\n",
      "\n",
      "AWS 및 기타 퍼블릭 클라우드 공급업체(예: Google Cloud Platform(GCP) 및 Microsoft Azure)는 하드웨어와 인프라를 관리 및 유지 관리하여 조직과 개인이 현장에서 리소스를 구매하고 실행하는 데 드는 비용과 복잡성을 덜어줍니다. 이러한 리소스는 무료 또는 사용량 기반 유료로 액세스할 수 있습니다.\n",
      "\n",
      "AWS를 더 잘 이해하려면 A\n",
      "\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, trans in enumerate(transcripts):\n",
    "    print(f\"transcript #{i+1}: {trans[:300]}\\n\")\n",
    "    print(\"====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74faf4ac-d5ec-42b7-8f6b-b02988898b10",
   "metadata": {},
   "source": [
    "# Post Call Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27963a63-4eb6-4b40-a88c-9396ad202c7e",
   "metadata": {},
   "source": [
    "## Choice of models in Bedrock\n",
    "Choose FMs from Amazon, AI21 Labs and Anthropic to find the right FM for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94025e02-4dc2-48a4-8324-171b906769d4",
   "metadata": {},
   "source": [
    "**Select region: \"us-east-1\"(M1), \"us-west-2\"(M2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ef184e65-8202-42f2-86dc-734739d8e1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_region = \"us-east-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "14312ebe-27a4-4f83-9971-1f49637ad741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bedrock_region == \"us-east-1\":    \n",
    "    bedrock_config = {\n",
    "        \"region_name\":bedrock_region,\n",
    "        \"endpoint_url\":\"https://bedrock.us-east-1.amazonaws.com\"\n",
    "    }\n",
    "elif bedrock_region == \"us-west-2\":  \n",
    "    bedrock_config = {\n",
    "        \"region_name\":bedrock_region,\n",
    "        \"endpoint_url\":\"https://prod.us-west-2.frontend.bedrock.aws.dev\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9a00ed0d-3400-403f-a40d-60bf22204ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name=bedrock_config[\"region_name\"],\n",
    "    endpoint_url=bedrock_config[\"endpoint_url\"]\n",
    ")\n",
    "\n",
    "bedrock_models = {\n",
    "    \"Claude\" : \"anthropic.claude-v1\",\n",
    "    \"TitanText\": \"amazon.titan-tg1-large\", \n",
    "    \"Claude-instant\":\"anthropic.claude-instant-v1\",\n",
    "    \"Claude-V2\" : \"anthropic.claude-v2\",\n",
    "}\n",
    "\n",
    "max_tokens = {\n",
    "    \"Claude\" : 12000,\n",
    "    \"TitanText\": 4096,\n",
    "    \"Claude-instant\": 9000,\n",
    "    \"Claude-V2\" : 12000,\n",
    "}\n",
    "\n",
    "max_tokens = {\"Claude\" : 120, \"TitanText\": 130, \"Claude-instant\": 120, \"Claude-V2\" : 120}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4355a638-0ecd-42e2-a98b-b62ae3022b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a3171a9f-feac-4c3a-8940-7f9bb40e5014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose one of the bedrock model\n",
    "model = \"Claude-V2\" # \"Claude\", \"TitanText\", \"Claude-instant\"\n",
    "if model in [\"Claude\", \"Claude-instant\", \"Claude-V2\"]:\n",
    "    llm = Bedrock(\n",
    "        model_id=bedrock_models[model],\n",
    "        client=bedrock,\n",
    "        model_kwargs={\n",
    "            \"max_tokens_to_sample\":512,\n",
    "            \"stop_sequences\":[\"\\n\\nhuman\", \"\\n\\n인간\", \"\\n\\n상담원\"],\n",
    "            \"temperature\":0,\n",
    "            \"top_p\":0.9\n",
    "        },\n",
    "        #endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev'\n",
    "    )\n",
    "elif model == \"TitanText\":\n",
    "    llm = Bedrock(\n",
    "        model_id=bedrock_models[model],\n",
    "        client=bedrock,\n",
    "        model_kwargs={\n",
    "            \"maxTokenCount\":4096,\n",
    "            \"stopSequences\":[],\n",
    "            \"temperature\":0,\n",
    "            \"topP\":0.9\n",
    "        },\n",
    "        #endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128dd79-f560-4278-82e7-19d1b2a16eac",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "In this notebook, we'll be performing four different analyses(**Summary, Sentiment, Intent and Resolution**), and we'll need a template for each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b2bd9304-f0f3-46f2-9414-58fbdc85b132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template = \"\"\"Analyze the retail support call transcript below. Provide a detail summary of the conversation in complete sentence:\n",
    "\n",
    "context: \"{transcript}\"\n",
    "\n",
    "summary:\"\"\"\n",
    "\n",
    "# What is the sentiment of the conversation: \"\"\"\n",
    "sentiment_template = \"\"\"\n",
    "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
    "[\"POSITIVE\", \"NEUTRAL\",\"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
    "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "sentiment: \"\"\"\n",
    "\n",
    "intent_template = \"\"\"This is a intent classification program. What is the purpose of the customer call use following \n",
    "classes [\"SHIPMENT_DELAY\", \"PRODUCT_DEFECT\", \"ACCOUNT_QUESTION\"]. Classify the conversation into one \n",
    "and exact one of these classes. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class. \n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Answer in one word, why is customer calling today: \"\"\"\n",
    "\n",
    "resolution_template = \"\"\"This is a resolution classification program. How did the agent solved the issue use following \n",
    "classes [\"FULL_REFUND\", \"PARTIAL_REFUND\",  \"QUESTION_ANSWERED\", \"UNRESOLVED\"]. classify the conversation into one \n",
    "and exact one of these classes. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Answer in one word, how did the agent resolve the customer question or issue: \"\"\"\n",
    "\n",
    "topic_template = \"\"\"This is topic identification program. What specific topic agent observed during the call. If you don't know, please say \"I don't know\". Do not try to make up.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Topic: \"\"\"\n",
    "\n",
    "escalation_template = \"\"\"This is escalation classification program. Did customer asked for escalation during the call. use following classes [\"YES\", \"NO\",  \"UNKNOWN\"]. Classify the conversation into one \n",
    "and exact one of these classes. Answer in one word without any explaination. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Escalation: \"\"\"\n",
    "\n",
    "\n",
    "holdup_template = \"\"\"This is delay identification program. Did agent put customer on hold during the call. \n",
    "If Yes, provide top reason concisely. If no wait or hold, then just say \"No Hold-up\".\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Top Reason: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d268c-ce6b-43a8-a2c9-a8a716eb9a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "74ceac2e-313a-4b3c-9cda-ca5359f68a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template_ko = \"\"\"\n",
    "아래의 리테일 지원 통화 기록을 분석하세요. 전체 문장으로 대화에 대한 자세한 요약을 제공하세요.\n",
    "\n",
    "통화: \"{transcript}\"\n",
    "\n",
    "요약:\"\"\"\n",
    "\n",
    "sentiment_template_ko = \"\"\"\n",
    "감성 분석 프로그램입니다. 다음 클래스를 이용하여 고객의 감성을 분류하세요. \n",
    "[\"긍정\", \"중립\", \"부정\"]. 대화를 이 클래스 중 한 가지로 정확하게 분류합니다. \n",
    "모르거나 확실하지 않은 경우 [\"중립\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요.\n",
    "\n",
    "대화: \"{transcript}\"\n",
    "\n",
    "고객 감성:\"\"\"\n",
    "\n",
    "intent_template_ko = \"\"\"\n",
    "이것은 의도 분류 프로그램입니다. 다음 대화에서 고개의 목적은 무엇입니까? \n",
    "클래스 [\"배송_지연\", \"제품_결함\", \"계정_질문\"]. 대화를 다음 클래스 중 하나로 분류합니다. \n",
    "이 클래스 중 하나에 정확히 일치합니다. 모르는 경우 [\"UNKNOWN\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요. \n",
    "\n",
    "대화: \"{transcript}\"\n",
    "\n",
    "고객 목적:\"\"\"\n",
    "\n",
    "resolution_template_ko = \"\"\"\n",
    "이것은 해결 분류 프로그램입니다. 상담원이 문제를 해결한 방법은 다음과 같습니다. \n",
    "클래스 [\"FULL_REFUND\", \"PARTIAL_REFUND\", \"QUESTION_ANSWERED\", \"UNRESOLVED\"]. 대화를 다음 중 하나로 분류합니다. \n",
    "그리고 이 클래스 중 하나를 정확하게 분류하세요. 모르는 경우 [\"UNKNOWN\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요.\n",
    "\n",
    "대화: \"{transcript}\"\n",
    "\n",
    "상담원이 고객의 질문이나 문제를 어떻게 해결했는지 한 마디로 답하세요:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f54dae-6f83-4820-9764-d8e9b943f8cc",
   "metadata": {},
   "source": [
    "## Generate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "84d5fe77-773c-4b3c-b146-068be5ddade0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_analysis(llm, transcript, max_tokens=50, template=\"\"):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    \n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    print (analysis_prompt)\n",
    "        \n",
    "    analysis = llm(analysis_prompt)\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedaaac-c4ea-40b5-a4e8-02b6f913ebc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "062a9a46-2a69-4e9f-a3c8-1c25758b0026",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "아래의 리테일 지원 통화 기록을 분석하세요. 전체 문장으로 대화에 대한 자세한 요약을 제공하세요.\n",
      "\n",
      "통화: \"timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "상담원: 리테일 지원 라인에 전화해 주셔서 감사합니다. 제 이름은 ABC입니다. 오늘 무엇을 도와드릴까요?\n",
      "\n",
      "고객님: 예, 결함이 있는 제품을 받았는데 매우 화가 납니다! 이것은 용납할 수 없는 일이며 즉시 해결하고 싶습니다!\n",
      "\n",
      "상담원: 네: 결함이 있는 제품을 받으셨다니 유감입니다. 어떤 문제인지 알려주시겠어요?\n",
      "\n",
      "고객: 네: 제가 받은 제품이 파손되어 사용할 수 없습니다. 많은 돈을 주고 샀는데 이제 사용할 수도 없습니다! 이것은 용납할 수 없는 일이며 지금 당장 해결책을 요구합니다!\n",
      "\n",
      "상담원님: 불편을 끼쳐 드린 점 죄송하게 생각합니다. 제가 이 문제를 조사할 수 있도록 주문 번호를 알려주시겠어요?\n",
      "\n",
      "고객: 2357894561\n",
      "\n",
      "상담원: 감사합니다. 제품 결함에 대해 유감스럽게 생각합니다. 전액 환불해 드릴 수 있습니다. 괜찮으시겠어요?\n",
      "\n",
      "고객: 네: 예, 환불은 가능하지만 애초에 결함이 있는 제품을 받은 것이 매우 실망스러워요.\n",
      "\n",
      "상담원: 네: 실망하신 점 이해하며, 불편을 끼쳐 드려 죄송합니다. 즉시 환불을 처리해 드리겠습니다. 환불 확인서를 보내드릴 수 있도록 이메일 주소를 알려주시겠습니까?\n",
      "\n",
      "고객: 123@456.com\n",
      "\n",
      "상담원: 감사합니다. 환불을 처리했으며 24시간 이내에 확인 이메일을 받으실 수 있습니다. 오늘 제가 더 도와드릴 일이 있나요?\n",
      "\n",
      "고객: 아니요, 그게 다입니다. 환불해 주셔서 감사하지만 받은 제품에 여전히 매우 실망했습니다.\n",
      "\n",
      "상담원: 네: 이해하며, 다시 한 번 불편을 끼쳐 드려 죄송합니다. 소매 지원 라인에 문의해 주셔서 감사드리며 좋은 하루 되세요.\"\n",
      "\n",
      "요약:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 고객님은 온라인 주문을 통해 받은 제품이 파손되어 사용할 수 없다며 불만을 표시하셨습니다. 많은 돈을 지불했음에도 불구하고 결함 있는 제품을 받은 것에 대해 매우 화가 나 있으며 즉각적인 해결을 요구하셨습니다. 상담원은 유감을 표명하고 주문 번호를 확인한 후 전액 환불을 제안하였습니다. 고객님은 환불은 가능하나 결함 있는 제품을 받은 것에 대한 실망감을 표현하셨습니다. 상담원은 다시 한 번 사과하고 환불을 진행하겠다고 안내하였습니다. 전반적으로, 고객님은 받은 제품의 품질에 대해 매우 불만족하셨으나 상담원은 환불을 통해 문제를 해결하려 노력하였습니다.'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    template=summary_template_ko\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b317a2-f6ca-4c4d-981f-a8a29745e1ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6ca68894-4069-41ac-a427-ab30190a5901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "감성 분석 프로그램입니다. 다음 클래스를 이용하여 고객의 감성을 분류하세요. \n",
      "[\"긍정\", \"중립\", \"부정\"]. 대화를 이 클래스 중 한 가지로 정확하게 분류합니다. \n",
      "모르거나 확실하지 않은 경우 [\"중립\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요.\n",
      "\n",
      "대화: \"timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "상담원: 리테일 지원 라인에 전화해 주셔서 감사합니다. 제 이름은 ABC입니다. 오늘 무엇을 도와드릴까요?\n",
      "\n",
      "고객님: 예, 결함이 있는 제품을 받았는데 매우 화가 납니다! 이것은 용납할 수 없는 일이며 즉시 해결하고 싶습니다!\n",
      "\n",
      "상담원: 네: 결함이 있는 제품을 받으셨다니 유감입니다. 어떤 문제인지 알려주시겠어요?\n",
      "\n",
      "고객: 네: 제가 받은 제품이 파손되어 사용할 수 없습니다. 많은 돈을 주고 샀는데 이제 사용할 수도 없습니다! 이것은 용납할 수 없는 일이며 지금 당장 해결책을 요구합니다!\n",
      "\n",
      "상담원님: 불편을 끼쳐 드린 점 죄송하게 생각합니다. 제가 이 문제를 조사할 수 있도록 주문 번호를 알려주시겠어요?\n",
      "\n",
      "고객: 2357894561\n",
      "\n",
      "상담원: 감사합니다. 제품 결함에 대해 유감스럽게 생각합니다. 전액 환불해 드릴 수 있습니다. 괜찮으시겠어요?\n",
      "\n",
      "고객: 네: 예, 환불은 가능하지만 애초에 결함이 있는 제품을 받은 것이 매우 실망스러워요.\n",
      "\n",
      "상담원: 네: 실망하신 점 이해하며, 불편을 끼쳐 드려 죄송합니다. 즉시 환불을 처리해 드리겠습니다. 환불 확인서를 보내드릴 수 있도록 이메일 주소를 알려주시겠습니까?\n",
      "\n",
      "고객: 123@456.com\n",
      "\n",
      "상담원: 감사합니다. 환불을 처리했으며 24시간 이내에 확인 이메일을 받으실 수 있습니다. 오늘 제가 더 도와드릴 일이 있나요?\n",
      "\n",
      "고객: 아니요, 그게 다입니다. 환불해 주셔서 감사하지만 받은 제품에 여전히 매우 실망했습니다.\n",
      "\n",
      "상담원: 네: 이해하며, 다시 한 번 불편을 끼쳐 드려 죄송합니다. 소매 지원 라인에 문의해 주셔서 감사드리며 좋은 하루 되세요.\"\n",
      "\n",
      "고객 감성:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 부정\\n\\n고객의 감성은 대화 내용을 보면 분명히 부정적입니다. 고객은 결함 있는 제품을 받고 매우 화가 났으며, 이에 대한 해결책을 강력히 요구했습니다. 상담원이 환불을 제안했지만, 고객은 여전히 실망하고 있음을 표현했습니다. 따라서 이 대화의 고객 감성은 부정적인 것으로 판단합니다.\\n\\ntimestamp: 2022-12-27 08:27:49.219717'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    template=sentiment_template_ko\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573494c0-4a7a-47c1-ae43-9e0aa741949f",
   "metadata": {},
   "source": [
    "### Intent Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8ce3de55-2eaf-4e2b-b98f-c6950324520e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "이것은 의도 분류 프로그램입니다. 다음 대화에서 고개의 목적은 무엇입니까? \n",
      "클래스 [\"배송_지연\", \"제품_결함\", \"계정_질문\"]. 대화를 다음 클래스 중 하나로 분류합니다. \n",
      "이 클래스 중 하나에 정확히 일치합니다. 모르는 경우 [\"UNKNOWN\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요. \n",
      "\n",
      "대화: \"timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "상담원: 리테일 지원 라인에 전화해 주셔서 감사합니다. 제 이름은 ABC입니다. 오늘 무엇을 도와드릴까요?\n",
      "\n",
      "고객님: 예, 결함이 있는 제품을 받았는데 매우 화가 납니다! 이것은 용납할 수 없는 일이며 즉시 해결하고 싶습니다!\n",
      "\n",
      "상담원: 네: 결함이 있는 제품을 받으셨다니 유감입니다. 어떤 문제인지 알려주시겠어요?\n",
      "\n",
      "고객: 네: 제가 받은 제품이 파손되어 사용할 수 없습니다. 많은 돈을 주고 샀는데 이제 사용할 수도 없습니다! 이것은 용납할 수 없는 일이며 지금 당장 해결책을 요구합니다!\n",
      "\n",
      "상담원님: 불편을 끼쳐 드린 점 죄송하게 생각합니다. 제가 이 문제를 조사할 수 있도록 주문 번호를 알려주시겠어요?\n",
      "\n",
      "고객: 2357894561\n",
      "\n",
      "상담원: 감사합니다. 제품 결함에 대해 유감스럽게 생각합니다. 전액 환불해 드릴 수 있습니다. 괜찮으시겠어요?\n",
      "\n",
      "고객: 네: 예, 환불은 가능하지만 애초에 결함이 있는 제품을 받은 것이 매우 실망스러워요.\n",
      "\n",
      "상담원: 네: 실망하신 점 이해하며, 불편을 끼쳐 드려 죄송합니다. 즉시 환불을 처리해 드리겠습니다. 환불 확인서를 보내드릴 수 있도록 이메일 주소를 알려주시겠습니까?\n",
      "\n",
      "고객: 123@456.com\n",
      "\n",
      "상담원: 감사합니다. 환불을 처리했으며 24시간 이내에 확인 이메일을 받으실 수 있습니다. 오늘 제가 더 도와드릴 일이 있나요?\n",
      "\n",
      "고객: 아니요, 그게 다입니다. 환불해 주셔서 감사하지만 받은 제품에 여전히 매우 실망했습니다.\n",
      "\n",
      "상담원: 네: 이해하며, 다시 한 번 불편을 끼쳐 드려 죄송합니다. 소매 지원 라인에 문의해 주셔서 감사드리며 좋은 하루 되세요.\"\n",
      "\n",
      "고객 목적:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 제품_결함'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[0], template=intent_template_ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d260018-1cb4-43be-8184-017ab1d5b103",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "aef3a5d7-43d9-49c7-87aa-90fb123facdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "이것은 해결 분류 프로그램입니다. 상담원이 문제를 해결한 방법은 다음과 같습니다. \n",
      "클래스 [\"FULL_REFUND\", \"PARTIAL_REFUND\", \"QUESTION_ANSWERED\", \"UNRESOLVED\"]. 대화를 다음 중 하나로 분류합니다. \n",
      "그리고 이 클래스 중 하나를 정확하게 분류하세요. 모르는 경우 [\"UNKNOWN\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요.\n",
      "\n",
      "대화: \"timestamp: 2022-12-28 08:26:49.219717\n",
      "\n",
      "상담원: 소매업체]에 전화해 주셔서 감사합니다. 제 이름은 [상담원 이름]입니다. 오늘은 무엇을 도와드릴까요?\n",
      "\n",
      "고객: 안녕하세요, 주문 상태를 확인하고 싶어서요. 오늘 도착하기로 되어 있었는데 아직 받지 못했습니다.\n",
      "\n",
      "상담원: 유감입니다. 주문 번호를 알려주시겠습니까?\n",
      "\n",
      "고객: 네, 123456입니다.\n",
      "\n",
      "상담원: 네: 감사합니다. 제가 확인해 보겠습니다. 창고에서 예기치 않은 문제가 발생하여 주문이 며칠 지연된 것 같습니다. 불편을 드려 죄송합니다.\n",
      "\n",
      "고객: 괜찮습니다. 그럴 수 있는 일이라는 것을 이해합니다. 그냥 상태가 궁금해서요.\n",
      "\n",
      "상담원: 네: 이해해 주셔서 감사합니다. 업데이트된 배송 날짜를 알려드릴까요?\n",
      "\n",
      "고객: 네, 그러세요.\n",
      "\n",
      "상담원: 네: 제가 가지고 있는 정보에 따르면 주문하신 상품은 [날짜]까지 도착할 예정입니다. 그러나 지연으로 인해 현재로서는 이를 보장할 수 없습니다.\n",
      "\n",
      "고객: 알겠습니다, 알려주셔서 감사합니다.\n",
      "\n",
      "상담원: 제가 더 도와드릴 일이 있나요?\n",
      "\n",
      "고객: 네: 네, 네. 지연에 대한 보상이 가능한지 궁금합니다.\n",
      "\n",
      "상담원: 네: 불편을 끼쳐 드려 다시 한 번 사과드립니다. 제가 어떻게 도와드릴 수 있는지 알아보겠습니다.\n",
      "\n",
      "고객: 감사합니다.\n",
      "\n",
      "상담원: 네: 주문과 지연을 검토한 후 [금액]의 일부 환불을 제공해 드릴 수 있습니다.\n",
      "\n",
      "고객: 네: 정말 관대하시네요! 정말 감사합니다.\n",
      "\n",
      "상담원: 네: 지연에 대한 최소한의 보상입니다. 더 도와드릴 일이 있나요?\n",
      "\n",
      "고객: 아니요, 그게 다입니다. 도와주셔서 감사드리며 문제를 해결해 주셔서 감사합니다.\n",
      "\n",
      "상담원: 네: 천만에요. 리테일러]를 선택해 주셔서 감사드리며 좋은 하루 되세요!\"\n",
      "\n",
      "상담원이 고객의 질문이나 문제를 어떻게 해결했는지 한 마디로 답하세요:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' PARTIAL_REFUND\\n\\n나: 이 대화에서 상담원은 고객의 주문 지연 문제를 확인하고 부분적인 환불을 제공함으로써 문제를 해결했습니다. 따라서 이 대화의 해결 방법은 PARTIAL_REFUND입니다.'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[6], template=resolution_template_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e7f27e-1823-462e-9a0a-576a69a6d50f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is topic identification program. What specific topic agent observed during the call. If you don't know, please say \"I don't know\". Do not try to make up.\n",
      "\n",
      "conversation: \"timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling our retail support line. My name is ABC. How can I assist you today?\n",
      "\n",
      "Customer: Yes, I have received a defective product, and I am extremely angry about it! This is unacceptable, and I want it resolved immediately!\n",
      "\n",
      "Agent: I'm sorry to hear that you received a defective product. Can you please let me know what the issue is?\n",
      "\n",
      "Customer: The product I received is broken and unusable. I spent a lot of money on it, and now I can't even use it! This is unacceptable, and I demand a solution right now!\n",
      "\n",
      "Agent: I completely understand your frustration, and I'm sorry for any inconvenience this has caused you. Can you please provide me with your order number so that I can look into this for you?\n",
      "\n",
      "Customer: 2357894561\n",
      "\n",
      "Agent: Thank you. I'm sorry to hear about the defective product you received. We can definitely offer you a full refund for it. Would that be acceptable to you?\n",
      "\n",
      "Customer: Yes, a refund would be acceptable, but I'm still very disappointed that I received a defective product in the first place.\n",
      "\n",
      "Agent: I understand your disappointment, and we apologize for any inconvenience this has caused you. We will process the refund for you right away. Can you please provide me with your email address so that we can send you a confirmation of the refund?\n",
      "\n",
      "Customer: 123@456.com\n",
      "\n",
      "Agent: Thank you. We have processed the refund, and you should receive the confirmation email within the next 24 hours. Is there anything else I can assist you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you for offering the refund, but I'm still very disappointed with the product I received.\n",
      "\n",
      "Agent: I understand, and once again, I apologize for any inconvenience this has caused you. Thank you for contacting our retail support line, and have a great day.\"\n",
      "\n",
      "Topic: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Defective product'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[0], template=topic_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3ea98fb-aac2-4cc9-8d2c-7d3e5ad1ffaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is escalation classification program. Did customer asked for escalation during the call. use following classes [\"YES\", \"NO\",  \"UNKNOWN\"]. Classify the conversation into one \n",
      "and exact one of these classes. Answer in one word without any explaination. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "Escalation: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NO'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=escalation_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "877be6ad-546e-4919-b434-468076faaff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is delay identification program. Did agent put customer on hold during the call. \n",
      "If Yes, provide top reason concisely. If no wait or hold, then just say \"No Hold-up\".\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "Top Reason: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No Hold-up'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=holdup_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988ceca-2edb-4bff-93fd-b6730c0d54e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling long call transcripts\n",
    "We'll cover how to handle long transcripts that exceed the limits of the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b86ca5e-971b-4034-a5ed-99f4292227bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if exceeds titan limit of 4000 tokens; Chunk it up\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb786d0-c847-4f37-bbc2-54b2d33e8f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Analysis\n",
    "def generate_analysis(llm, transcript, template=\"\", model=\"Claude\"):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    \n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    \n",
    "    num_tokens = llm.get_num_tokens(analysis_prompt)\n",
    "    print (f'prompt has almost {num_tokens} tokens \\n')\n",
    "    \n",
    "    print (\"max_tokens[model]\", max_tokens[model])\n",
    "    if num_tokens > max_tokens[model]:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=20\n",
    "        )\n",
    "        docs = text_splitter.create_documents([transcript])\n",
    "        \n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "        print(\n",
    "            f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\"\n",
    "        )\n",
    "        summary_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=True) # map_reduce\n",
    "        \n",
    "        transcript = summary_chain.run(docs)\n",
    "                \n",
    "    analysis_prompt = prompt.format(transcript=transcript) \n",
    "    analysis = llm(analysis_prompt)\n",
    "    \n",
    "    return (analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ef1e7-4687-43a0-9935-2d4ac263285b",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c23fa62-0898-4bb7-9991-d2d4705e0a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt has almost 452 tokens \n",
      "\n",
      "max_tokens[model] 120\n",
      "Now we have 5 documents and the first one has 101 tokens\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"timestamp: 2022-12-27 08:26:49.219717\n",
      "\n",
      "Agent: Thank you for calling our retail support line. My name is ABC. How can I assist you today?\n",
      "\n",
      "Customer: Yes, I have received a defective product, and I am extremely angry about it! This is unacceptable, and I want it resolved immediately!\n",
      "\n",
      "Agent: I'm sorry to hear that you received a defective product. Can you please let me know what the issue is?\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "Customer calls retail support line to report defective product and demands immediate resolution. Agent apologizes and asks for details about the issue.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Customer: The product I received is broken and unusable. I spent a lot of money on it, and now I can't even use it! This is unacceptable, and I demand a solution right now!\n",
      "\n",
      "Agent: I completely understand your frustration, and I'm sorry for any inconvenience this has caused you. Can you please provide me with your order number so that I can look into this for you?\n",
      "\n",
      "Customer: 2357894561\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "Customer calls retail support line to report defective product and demands immediate resolution. Agent asks for order number and looks into issue.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Agent: Thank you. I'm sorry to hear about the defective product you received. We can definitely offer you a full refund for it. Would that be acceptable to you?\n",
      "\n",
      "Customer: Yes, a refund would be acceptable, but I'm still very disappointed that I received a defective product in the first place.\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "Agent: Thank you. I'm sorry to hear about the defective product you received. We can definitely offer you a full refund for it. Would that be acceptable to you?\n",
      "\n",
      "Customer: Yes, a refund would be acceptable, but I'm still very disappointed that I received a defective product in the first place.\n",
      "\n",
      "Agent: I completely understand. We take product quality very seriously, and I apologize for any inconvenience this may have caused you. Can you please provide me with your order number so I can look into this issue further?\n",
      "\n",
      "Customer: Sure, my order number is 123456789.\n",
      "\n",
      "Agent: Thank you. I have located your order in our system. It appears that you received a defective battery charger. We will send you a prepaid shipping label so you can return the item to us for a full refund. We will also issue a replacement battery charger to you at no additional cost.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Agent: I understand your disappointment, and we apologize for any inconvenience this has caused you. We will process the refund for you right away. Can you please provide me with your email address so that we can send you a confirmation of the refund?\n",
      "\n",
      "Customer: 123@456.com\n",
      "\n",
      "Agent: Thank you. We have processed the refund, and you should receive the confirmation email within the next 24 hours. Is there anything else I can assist you with today?\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "Customer: Thank you for your help. I appreciate the refund, but I'm still concerned about the quality of your products.\n",
      "\n",
      "Agent: I understand your concerns. We take product quality very seriously, and I apologize for any inconvenience this may have caused you. Rest assured that we are taking steps to ensure that this doesn't happen again in the future.\n",
      "\n",
      "Customer: That's good to hear, but I hope you'll also consider offering some kind of compensation for the trouble this has caused me.\n",
      "\n",
      "Agent: We appreciate your feedback, and we will definitely consider that. Can you please provide me with your feedback in the form of a short survey?\n",
      "\n",
      "Customer: Sure, I can do that.\n",
      "\n",
      "Agent: Thank you. We value your feedback and use it to improve our customer service. Is there anything else I can assist you with today?\n",
      "\n",
      "Customer: No, that's all for now. Thanks again.\n",
      "\n",
      "Agent: You're welcome. Have a great day!\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Customer: No, that's all. Thank you for offering the refund, but I'm still very disappointed with the product I received.\n",
      "\n",
      "Agent: I understand, and once again, I apologize for any inconvenience this has caused you. Thank you for contacting our retail support line, and have a great day.\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The customer contacts customer service to express concern about the quality of a product they purchased. The agent apologizes and offers a refund, and also mentions that the company is taking steps to ensure that this doesn't happen again in the future. The customer requests some form of compensation for the inconvenience, and the agent asks them to complete a short survey. The customer agrees to complete the survey and thanks the agent for their assistance. The agent assures the customer that they value their feedback and asks if there is anything else they can assist with. The customer declines additional assistance and thanks the agent again before ending the call.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[0], template=summary_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187afa8f-908c-4821-ac77-29eac133d6c9",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76584745-17c0-445b-b852-a6779f110ff7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
      "[\"POSITIVE\", \"NEUTRAL\",\"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
      "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class.\n",
      "\n",
      "conversation: \"timestamp: 2023-01-28 08:26:49.219717\n",
      "\n",
      "Customer: Hi, I'd like to check the balance on my account.\n",
      "\n",
      "Retail Support: Sure thing! Can I have your account number or phone number associated with the account?\n",
      "\n",
      "Customer: My phone number is (123) 456-7890.\n",
      "\n",
      "Retail Support: Great, thank you. Let me pull up your account. Okay, it looks like your current balance is $567.89.\n",
      "\n",
      "Customer: Okay, great. Thank you.\n",
      "\n",
      "Retail Support: You're welcome! Is there anything else I can help you with today?\n",
      "\n",
      "Customer: No, that's all. Thank you.\n",
      "\n",
      "Retail Support: Not a problem. Thank you for calling. Have a great day!\n",
      "\n",
      "Customer: You too. Goodbye.\n",
      "\n",
      "Retail Support: Goodbye!\"\n",
      "\n",
      "sentiment: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error raised by bedrock service: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mThrottlingException\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/bedrock.py:189\u001b[0m, in \u001b[0;36mBedrock._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontentType\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     text \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(provider, response)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    963\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mThrottlingException\u001b[0m: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscript\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranscripts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentiment_template\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 8\u001b[0m, in \u001b[0;36mgenerate_analysis\u001b[0;34m(llm, transcript, max_tokens, template)\u001b[0m\n\u001b[1;32m      5\u001b[0m analysis_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(transcript\u001b[38;5;241m=\u001b[39mtranscript)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (analysis_prompt)\n\u001b[0;32m----> 8\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalysis_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:427\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    437\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:279\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[1;32m    276\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    277\u001b[0m         dumpd(\u001b[38;5;28mself\u001b[39m), prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[1;32m    278\u001b[0m     )\n\u001b[0;32m--> 279\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:223\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    222\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    224\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:210\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    202\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 210\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    219\u001b[0m         )\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/base.py:602\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m    601\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 602\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/llms/bedrock.py:195\u001b[0m, in \u001b[0;36mBedrock._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     text \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(provider, response)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by bedrock service: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     text \u001b[38;5;241m=\u001b[39m enforce_stop_tokens(text, stop)\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by bedrock service: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again."
     ]
    }
   ],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=sentiment_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee26221-a4e5-4e3e-896e-5c9d40080900",
   "metadata": {},
   "source": [
    "### Intent Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0bf4f-cdb1-4f3d-a685-f6966f45223e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm,transcript=transcripts[1], template=intent_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffde34e-8fec-4a7d-86be-ca6a580d151d",
   "metadata": {},
   "source": [
    "### Resolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cc2ac-4e00-440a-9056-9809d4064cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=resolution_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89141b59-f8cb-4a0c-b5f3-2d076a09f949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
