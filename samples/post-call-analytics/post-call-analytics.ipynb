{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df503520-00ac-434c-aea7-458add4857d9",
   "metadata": {},
   "source": [
    "## Post Call Analytics (PCA) Using Amazon Bedrock\n",
    "\n",
    "Welcome to this training module on post-call analytics use cases using Amazon Bedrock. \n",
    "\n",
    "As businesses continue to interact with customers through various channels, it becomes increasingly important to analyze these interactions to gain insights into customer behavior and preferences. Post-call analytics is one such method that involves analyzing customer interactions after the call has ended. The use of large language models can greatly enhance the effectiveness of post-call analytics by enabling more accurate sentiment analysis, identifying specific customer needs and preferences, and improving overall customer experience. \n",
    "\n",
    "In this sample notebook, we will explore following topics to demonstrate the various benefits of using Bedrock for post-call analytics and businesses gain a competitive edge in the modern marketplace.\n",
    "\n",
    "- Choice of LLM models in Bedrock (Titan Text and Anthropic Claude)\n",
    "- One model handling multiple PCA tasks\n",
    "- Handling long call transcripts\n",
    "- [Stretch] Architecture pattern for production workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d1935-a875-4938-9c1f-ac171dedb608",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "Install and upgrade the packages required to run the sample code. <BR>\n",
    "**Note: you may need to restart the kernel to use updated packages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4af435-2f0b-47cf-849c-fc16f8189400",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547efff-ad04-471e-af4d-d7988781d23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import PromptTemplate\n",
    "import boto3\n",
    "\n",
    "print(f\"langchain version check: {langchain.__version__}\")\n",
    "print(f\"boto3 version check: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8351fe-cf0b-46ac-bd71-fac01c47f9fd",
   "metadata": {},
   "source": [
    "# Load transcript files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c458a-7cf2-4029-90fd-a641f1556dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_files = [\n",
    "    #\"./call_transcripts/negative-refund.txt\", \n",
    "    #\"./call_transcripts/neutral-short.txt\",\n",
    "    #\"./call_transcripts/positive-partial-refund.txt\",\n",
    "    #\"./call_transcripts/aws.txt\",\n",
    "    \"./call_transcripts/negative-refund-ko.txt\",\n",
    "    \"./call_transcripts/neutral-short-ko.txt\",\n",
    "    \"./call_transcripts/positive-partial-refund-ko.txt\",\n",
    "    \"./call_transcripts/aws-ko.txt\"\n",
    "]\n",
    "\n",
    "transcripts = []\n",
    "\n",
    "for file_name in transcript_files:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        transcripts.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237aa06-a992-4713-bd21-6b82b0d763a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, trans in enumerate(transcripts):\n",
    "    print(f\"transcript #{i+1}: {trans[:300]}\\n\")\n",
    "    print(\"====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74faf4ac-d5ec-42b7-8f6b-b02988898b10",
   "metadata": {},
   "source": [
    "# Post Call Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27963a63-4eb6-4b40-a88c-9396ad202c7e",
   "metadata": {},
   "source": [
    "## Choice of models in Bedrock\n",
    "Choose FMs from Amazon, AI21 Labs and Anthropic to find the right FM for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94025e02-4dc2-48a4-8324-171b906769d4",
   "metadata": {},
   "source": [
    "**Select region: \"us-east-1\"(M1), \"us-west-2\"(M2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef184e65-8202-42f2-86dc-734739d8e1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_region = \"us-east-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14312ebe-27a4-4f83-9971-1f49637ad741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bedrock_region == \"us-east-1\":    \n",
    "    bedrock_config = {\n",
    "        \"region_name\":bedrock_region,\n",
    "        \"endpoint_url\":\"https://bedrock.us-east-1.amazonaws.com\"\n",
    "    }\n",
    "elif bedrock_region == \"us-west-2\":  \n",
    "    bedrock_config = {\n",
    "        \"region_name\":bedrock_region,\n",
    "        \"endpoint_url\":\"https://prod.us-west-2.frontend.bedrock.aws.dev\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00ed0d-3400-403f-a40d-60bf22204ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name=bedrock_config[\"region_name\"],\n",
    "    endpoint_url=bedrock_config[\"endpoint_url\"]\n",
    ")\n",
    "\n",
    "bedrock_models = {\n",
    "    \"Claude\" : \"anthropic.claude-v1\",\n",
    "    \"TitanText\": \"amazon.titan-tg1-large\", \n",
    "    \"Claude-instant\":\"anthropic.claude-instant-v1\",\n",
    "    \"Claude-V2\" : \"anthropic.claude-v2\",\n",
    "}\n",
    "\n",
    "max_tokens = {\n",
    "    \"Claude\" : 12000,\n",
    "    \"TitanText\": 4096,\n",
    "    \"Claude-instant\": 9000,\n",
    "    \"Claude-V2\" : 12000,\n",
    "}\n",
    "\n",
    "max_tokens = {\"Claude\" : 120, \"TitanText\": 130, \"Claude-instant\": 120, \"Claude-V2\" : 120}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355a638-0ecd-42e2-a98b-b62ae3022b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3171a9f-feac-4c3a-8940-7f9bb40e5014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose one of the bedrock model\n",
    "model = \"Claude-V2\" # \"Claude\", \"TitanText\", \"Claude-instant\"\n",
    "if model in [\"Claude\", \"Claude-instant\", \"Claude-V2\"]:\n",
    "    llm = Bedrock(\n",
    "        model_id=bedrock_models[model],\n",
    "        client=bedrock,\n",
    "        model_kwargs={\n",
    "            \"max_tokens_to_sample\":512,\n",
    "            \"stop_sequences\":[\"\\n\\nhuman\", \"\\n\\n인간\", \"\\n\\n상담원\"],\n",
    "            \"temperature\":0,\n",
    "            \"top_p\":0.9\n",
    "        },\n",
    "        #endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev'\n",
    "    )\n",
    "elif model == \"TitanText\":\n",
    "    llm = Bedrock(\n",
    "        model_id=bedrock_models[model],\n",
    "        client=bedrock,\n",
    "        model_kwargs={\n",
    "            \"maxTokenCount\":4096,\n",
    "            \"stopSequences\":[],\n",
    "            \"temperature\":0,\n",
    "            \"topP\":0.9\n",
    "        },\n",
    "        #endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128dd79-f560-4278-82e7-19d1b2a16eac",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "In this notebook, we'll be performing four different analyses(**Summary, Sentiment, Intent and Resolution**), and we'll need a template for each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd9304-f0f3-46f2-9414-58fbdc85b132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template = \"\"\"Analyze the retail support call transcript below. Provide a detail summary of the conversation in complete sentence:\n",
    "\n",
    "context: \"{transcript}\"\n",
    "\n",
    "summary:\"\"\"\n",
    "\n",
    "# What is the sentiment of the conversation: \"\"\"\n",
    "sentiment_template = \"\"\"\n",
    "This is a sentiment analysis program. What is the customer sentiment using following classes \n",
    "[\"POSITIVE\", \"NEUTRAL\",\"NEGATIVE\"]. classify the conversation into one and exact one of these classes. \n",
    "If you don't know or not sure, please use [\"NEUTRAL\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "sentiment: \"\"\"\n",
    "\n",
    "intent_template = \"\"\"This is a intent classification program. What is the purpose of the customer call use following \n",
    "classes [\"SHIPMENT_DELAY\", \"PRODUCT_DEFECT\", \"ACCOUNT_QUESTION\"]. Classify the conversation into one \n",
    "and exact one of these classes. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class. \n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Answer in one word, why is customer calling today: \"\"\"\n",
    "\n",
    "resolution_template = \"\"\"This is a resolution classification program. How did the agent solved the issue use following \n",
    "classes [\"FULL_REFUND\", \"PARTIAL_REFUND\",  \"QUESTION_ANSWERED\", \"UNRESOLVED\"]. classify the conversation into one \n",
    "and exact one of these classes. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Answer in one word, how did the agent resolve the customer question or issue: \"\"\"\n",
    "\n",
    "topic_template = \"\"\"This is topic identification program. What specific topic agent observed during the call. If you don't know, please say \"I don't know\". Do not try to make up.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Topic: \"\"\"\n",
    "\n",
    "escalation_template = \"\"\"This is escalation classification program. Did customer asked for escalation during the call. use following classes [\"YES\", \"NO\",  \"UNKNOWN\"]. Classify the conversation into one \n",
    "and exact one of these classes. Answer in one word without any explaination. If you don't know, please use [\"UNKNOWN\"] class. Do not try to make up a class.\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Escalation: \"\"\"\n",
    "\n",
    "\n",
    "holdup_template = \"\"\"This is delay identification program. Did agent put customer on hold during the call. \n",
    "If Yes, provide top reason concisely. If no wait or hold, then just say \"No Hold-up\".\n",
    "\n",
    "conversation: \"{transcript}\"\n",
    "\n",
    "Top Reason: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d268c-ce6b-43a8-a2c9-a8a716eb9a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceac2e-313a-4b3c-9cda-ca5359f68a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template_ko = \"\"\"\n",
    "아래의 리테일 지원 통화 기록을 분석하세요. 전체 문장으로 대화에 대한 자세한 요약을 제공하세요.\n",
    "\n",
    "통화: \"{transcript}\"\n",
    "\n",
    "요약:\"\"\"\n",
    "\n",
    "sentiment_template_ko = \"\"\"\n",
    "감성 분석 프로그램입니다. 다음 클래스를 이용하여 고객의 감성을 분류하세요. \n",
    "[\"긍정\", \"중립\", \"부정\"]. 대화를 이 클래스 중 한 가지로 정확하게 분류합니다. \n",
    "모르거나 확실하지 않은 경우 [\"중립\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요.\n",
    "\n",
    "대화: \"{transcript}\"\n",
    "\n",
    "고객 감성:\"\"\"\n",
    "\n",
    "intent_template_ko = \"\"\"\n",
    "이것은 의도 분류 프로그램입니다. 다음 대화에서 고개의 목적은 무엇입니까? \n",
    "클래스 [\"배송_지연\", \"제품_결함\", \"계정_질문\"]. 대화를 다음 클래스 중 하나로 분류합니다. \n",
    "이 클래스 중 하나에 정확히 일치합니다. 모르는 경우 [\"UNKNOWN\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요. \n",
    "\n",
    "대화: \"{transcript}\"\n",
    "\n",
    "고객 목적:\"\"\"\n",
    "\n",
    "resolution_template_ko = \"\"\"\n",
    "이것은 해결 분류 프로그램입니다. 상담원이 문제를 해결한 방법은 다음과 같습니다. \n",
    "클래스 [\"FULL_REFUND\", \"PARTIAL_REFUND\", \"QUESTION_ANSWERED\", \"UNRESOLVED\"]. 대화를 다음 중 하나로 분류합니다. \n",
    "그리고 이 클래스 중 하나를 정확하게 분류하세요. 모르는 경우 [\"UNKNOWN\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요.\n",
    "\n",
    "대화: \"{transcript}\"\n",
    "\n",
    "상담원이 고객의 질문이나 문제를 어떻게 해결했는지 한 마디로 답하세요:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f54dae-6f83-4820-9764-d8e9b943f8cc",
   "metadata": {},
   "source": [
    "## Generate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5fe77-773c-4b3c-b146-068be5ddade0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_analysis(llm, transcript, max_tokens=50, template=\"\"):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    \n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    print (analysis_prompt)\n",
    "        \n",
    "    analysis = llm(analysis_prompt)\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedaaac-c4ea-40b5-a4e8-02b6f913ebc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a9a46-2a69-4e9f-a3c8-1c25758b0026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    template=summary_template_ko\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b317a2-f6ca-4c4d-981f-a8a29745e1ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca68894-4069-41ac-a427-ab30190a5901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(\n",
    "    llm=llm,\n",
    "    transcript=transcripts[0],\n",
    "    template=sentiment_template_ko\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573494c0-4a7a-47c1-ae43-9e0aa741949f",
   "metadata": {},
   "source": [
    "### Intent Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3de55-2eaf-4e2b-b98f-c6950324520e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[0], template=intent_template_ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d260018-1cb4-43be-8184-017ab1d5b103",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3a5d7-43d9-49c7-87aa-90fb123facdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[6], template=resolution_template_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7f27e-1823-462e-9a0a-576a69a6d50f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[0], template=topic_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea98fb-aac2-4cc9-8d2c-7d3e5ad1ffaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=escalation_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877be6ad-546e-4919-b434-468076faaff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=holdup_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988ceca-2edb-4bff-93fd-b6730c0d54e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling long call transcripts\n",
    "We'll cover how to handle long transcripts that exceed the limits of the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86ca5e-971b-4034-a5ed-99f4292227bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if exceeds titan limit of 4000 tokens; Chunk it up\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb786d0-c847-4f37-bbc2-54b2d33e8f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Analysis\n",
    "def generate_analysis(llm, transcript, template=\"\", model=\"Claude\"):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    \n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    \n",
    "    num_tokens = llm.get_num_tokens(analysis_prompt)\n",
    "    print (f'prompt has almost {num_tokens} tokens \\n')\n",
    "    \n",
    "    print (\"max_tokens[model]\", max_tokens[model])\n",
    "    if num_tokens > max_tokens[model]:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=20\n",
    "        )\n",
    "        docs = text_splitter.create_documents([transcript])\n",
    "        \n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "        print(\n",
    "            f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\"\n",
    "        )\n",
    "        summary_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=True) # map_reduce\n",
    "        \n",
    "        transcript = summary_chain.run(docs)\n",
    "                \n",
    "    analysis_prompt = prompt.format(transcript=transcript) \n",
    "    analysis = llm(analysis_prompt)\n",
    "    \n",
    "    return (analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ef1e7-4687-43a0-9935-2d4ac263285b",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23fa62-0898-4bb7-9991-d2d4705e0a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[0], template=summary_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187afa8f-908c-4821-ac77-29eac133d6c9",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76584745-17c0-445b-b852-a6779f110ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=sentiment_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee26221-a4e5-4e3e-896e-5c9d40080900",
   "metadata": {},
   "source": [
    "### Intent Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0bf4f-cdb1-4f3d-a685-f6966f45223e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm,transcript=transcripts[1], template=intent_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffde34e-8fec-4a7d-86be-ca6a580d151d",
   "metadata": {},
   "source": [
    "### Resolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cc2ac-4e00-440a-9056-9809d4064cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_analysis(llm=llm, transcript=transcripts[1], template=resolution_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89141b59-f8cb-4a0c-b5f3-2d076a09f949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
